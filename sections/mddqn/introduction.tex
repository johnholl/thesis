The driver management 
system at a ride-sharing company must make decisions both for assigning available drivers to nearby unassigned passengers (hereby called orders) over a large spatial decision-making region (e.g., a city) and for repositioning drivers who have no nearby orders. Such decisions not only have immediate to short-term impact in the form of revenue from assigned orders and driver availability, but also long-term effects on the distribution of available drivers across the city. This distribution critically affects how well future orders can be served. The need to address the exploration-exploitation dilemma as well as the delayed consequences of assignment actions makes this a Reinforcement Learning (RL) problem.

Recent works \cite{bello2016neural,nazari2018deep} have successfully applied Deep Reinforcement Learning techniques to dispatching problems, such as the Traveling Salesman Problem (TSP) and the more general Vehicle Routing Problem (VRP), however they have primarily focused on {\em static} (i.e., those where all orders are known up front) and/or {\em single-driver} dispatching problems. In contrast, for use in ride-sharing applications, one needs to find good policies that can accommodate a {\em multi-driver} dispatching scheme where demands are not known up front but rather generated {\em dynamically} throughout an episode (e.g., a day). We refer to 
%the above decision problem as a 
this problem as a multi-driver vehicle dispatching and repositioning problem (MDVDRP). 

%Next we define the mathematical abstraction of MDVDRP we use in this paper. 
We define an MDVDRP as a continuous-time semi-Markov decision process with the following state-reward dynamics. At any time, state is given by a set of requesting {\em orders} $ o_t^i \in \mathcal{O}_t$, a set of drivers {\em drivers} $d_t^i \in \mathcal{D}_t$, and time of day $t$. There is also a set of repositioning movements $m^j \in \mathcal{M}$, which are not part of state but will be part of the action space. The size of $\mathcal{O}_t$ will change in time due to orders stochastically appearing in the environment and disappearing as they are served or canceled. Orders are canceled if they do not receive a driver assignment within a given time window. $\mathcal{D}_t$ can be further subdivided into {\em available drivers} and {\em unavailable drivers}. A driver is unavailable if and only if they are currently fulfilling an order. An action is a pairing of an available driver with either a requesting order or repositioning movement. An order is characterized by a pickup location (where the passenger is located), and end location (where the customer wants to go), a price, and the amount of time since the order arrived in the system. A driver is described by her position if she is available, and her paired order or reposition movement if she is unavailable. A reposition movement is described by a direction and duration, e.g. ``Travel west for three minutes''. When a driver is assigned to an order, the agent receives a reward equal to the price of that order, and the driver becomes unavailable until after it has picked up its order at the start location and dropped it off at the end location. When a driver is assigned a repositioning movement, the agent receives no reward and repositions until it either reaches the maximum repositioning duration or is assigned an order. When any action is taken in the environment, time advances to the next time that there is a driver that is available and not repositioning. Note that if there are multiple non-repositioning drivers at time $t$ and multiple requesting orders, then time will not advance after the first action is taken since there will still be at least one available non-repositioning driver and order pairing.

% \textcolor{red}{[I think you are referring to the case where there are also multiple orders pending assignments.  If there is only one order, then time has to advance to the next time when the next order arrives, and an additional driver gets assigned.]}

Heuristic solutions construct an approximation to the true problem by ignoring the spatial extent, or the temporal dynamics, or both, and solve the approximate problem exactly. One such example is myopic pickup distance minimization (MPDM), which ignores temporal dynamics and always assigns the closest available driver to a requesting order~\cite{zhang2017taxi}. We illustrate this below in two simple dispatching domains that capture the essence of these suboptimalities, and demonstrate that our methods can overcome such issues.

In this paper we construct a global state representation along with a neural network (NN) architecture that can take this global state as input and produce action-values (Q-values). Due to the variable number of orders, which appear both as part of state and part of actions, we make use of attention mechanisms both for input and output of the NN. We then present two methods for training this network to perform dispatching and repositioning: single-driver deep Q-learning network (SD-DQN) and multi-driver deep Q-learning network (MD-DQN). Both approaches are based on the deep Q-learning network (DQN) algorithm~\cite{mnih2015human}, but differ from each other in the extent to which individual driver behaviors are coordinated. SD-DQN learns a Q-value function of global state for single drivers by accumulating rewards along {\em single-driver} trajectories. On the other hand, MD-DQN uses {\em system-centric} trajectories, so that the Q-value function accumulates rewards for all drivers. We find that MD-DQN can learn superior behavior policies in some cases, but that in practice SD-DQN is competitive in all environments and scales well with the number of drivers in the MDVDRP while MD-DQN performs poorly in real-data domains. Empirically we compare performance of SD-DQN and MD-DQN on a static assignment problem, illustrative MDVDRPs, and a data-driven MDVDRP designed using real world ride-sharing dispatching data.