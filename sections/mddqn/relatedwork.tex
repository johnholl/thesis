\section{Related Work}

There have been several recent approaches to solving dispatching and routing problems. Some examples include Bello et al. (\cite{bello2016neural}), Nazari et al.~(\cite{nazari2018deep}), and Vinyals et al.~(\cite{vinyals2015pointer}). All of these works use an encoding-decoding scheme, where first information is processed into a global-context, and then from this context actions are decoded sequentially. Pointer networks \cite{vinyals2015pointer} offer an approximate solution to TSPs by encoding cities (in our terminology, orders) sequentially with a recurrent network, and then producing a solution by sequentially ``pointing'' to orders using an attention mechanism \cite{mnih2014recurrent}. The network is trained with a supervised loss function by imposing a fixed ordering rule on decoded orders. Bello et al.(\cite{bello2016neural}) build off of the pointer network architecture, but train the network with policy gradients and so may dispense with the fixed ordering during the decoding phase. 

% In Khalil et al.~(\cite{khalil2017learning}), the authors exploit the graph structure of such static problems, building solutions iteratively.

While related to our work, the above papers typically focus on problems that are {\em static} and/or {\em single-driver} (in the sense that there is only one driver that must be controlled). In contrast, we are interested in making dispatching decisions for many drivers in a dynamic environment. Furthermore, the sequential encoding in Bello et al.~and Vinyals et al. introduces an unnecessary forced ordering of inputs into the neural network by encoding using a recurrent network. Instead we follow an architecture more closely related to Nazari et al.(\cite{nazari2018deep}), which uses an attention mechanism \cite{mnih2014recurrent} for encoding, however they only apply their architecture to static and single-driver vehicle routing problems. We depart from their architecture further by dispensing with the recurrent network used for decoding. Instead, we construct a global representation of state that is encoded and decoded in a completely feed-forward fashion.

Next we discuss a previous work on a problem more closely related to MDVDRPs. Oda et al. \cite{oda2018movi} offer a {\em value-based} approach to the  dynamic fleet management problem, which is a strict subproblem of the MDVDRP. In dynamic fleet management, one is concerned only with {\em repositioning} available drivers, while driver-order assignments are handled via hard-coded rules (in their case, minimizing pickup distance). A MDVDRP combines fleet management with driver-order matching.

Another thread of related work comes from the multi-agent reinforcement learning literature. Specifically, our two training approaches, SD-DQN and MD-DQN, are analogous to the multi-agent reinforcement learning (MARL) algorithms of ``independent Q-learning'' \cite{claus1998dynamics} and ``team-Q learning'' \cite{littman2001value} respectively. The tradeoffs between these two approaches to team problems are broadly known but not well understood. Team-Q  has the benefit of theoretical justification, but its action set grows exponentially in the number of agents. On the other hand, while mostly lacking in theory, independent Q-learning has been leveraged successfully in a number of problems \cite{crites1998elevator,oda2018movi}. Claus and Boutilier (\cite{claus1998dynamics}) conjecture that
independent Q-learners will converge to a Nash equilibrium under appropriate assumptions in team games, however this claim remains unproven. Our results reflect the theme that SD-DQN works well in practice at a variety of scales in team problems despite the dearth of convergence results, while MD-DQN has considerable difficulty scaling beyond a small number of agents.