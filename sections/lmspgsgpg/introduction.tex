\section{Introduction}

In the previous chapter we defined two classes of stochastic games: stochastic potential games (SPGs) and stochastic global potential games (SGPGs). Via their analogies with normal form potential games, these two classes of games admit extra structure on their Nash equilibrium sets. Namely, both SPGs and SGPGs admit pure Nash equilibrium behaviors. Furthermore, each admits a distinguished class of Nash equilibrium. In the case of SPGs we called these ``simultaneously potential maximizing equilibria.'' SGPGs on the other hand admit ``potential maximizing equilibria,'' so named because they follow essentially the same definition as potential maximizing equilibria in normal form potential games. 

The structure of their Nash equilibrium sets immediately raise the question of whether there are convergent learning methods that converge to Nash equilibria, pure Nash equilibria, or the specialized Nash equilibria in each class of game. This chapter will focus on two learning methods: joint strategy fictitious play (JSFP) with inertia and log-linear learning (LLL). These two learning methods converge to pure Nash equilibria and potential maximizing equilibria respectively in potential games. For each of these learning methods we will examine how they may be extended to SPGs and SGPGs in order to define convergent learning dynamics in both. 