\section{Conclusion}

In this chapter we reviewed two learning methods that converge to Nash equilibrium in iterated potential games: JSFP with inertia and LLL. We were able to extend JSFP with inertia to SG-JSFP with inertia, a stochastic game learning algorithm that converges to pure Nash equilibria in both SPGs and SGPGs. We also extended LLL to an algorithm that converges to potential maximizing behavior in SGPGs. Finally, we discussed the obstruction to naively extending LLL to SPGs. This study has been far from comprehensive; with their added structure stochastic games offer a multitude of choices in designing learning algorithms. This chapter merely served as a first push into the study of learning in SPGs and SGPGs.