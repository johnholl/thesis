\section{Introduction}

In its early years, game theory focused on the static concepts of equilibria in normal-form games, proving their existence, and computing their behavior. From the period of the 1980's until the early 2000's there was a shift in research focus towards {\em learning} equilibria. This was motivated both by descriptive applications in economics as well as prescriptive applications in engineering. This led to a more in-depth study of simple learning rules such as best response, fictitious play, log-linear learning, and joint strategy fictitious play. A number of papers presented convergence results for special subclasses of games. Two types of games that are of particular interest are zero-sum games \cite{nash1951non} and potential games \cite{monderer1996potential}. Both classes of games admit pure Nash equilibria, and furthermore a number of the simple learning rules listed above converge to Nash equilibria in these types of games.

While a great deal of work has been done on learning to play equilibria strategies in normal-form games, the same cannot be said for stochastic games. The defining feature of stochastic games is that they introduce state dynamics, with each state corresponding to a normal-form ``stage'' game. Stochastic games are of interest both from a theoretical and practical point of view. They are the natural generalization of Markov decision processes (MDPs) to the multiagent setting. As such they extend the modelling capacity of MDPs, and can be used to model markets, bargaining, and routing problems for example.

In this chapter we will study two classes of stochastic games that are motivated by their connections to normal form potential games. They are {\em stochastic potential games} (SPGs) and {\em stochastic global potential games} (SGPGs).  We will study the properties of their equilibrium sets, transition dynamics, and stage games. In the next chapter we will follow this up by considering learning methods in these games.

