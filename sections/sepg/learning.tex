
\section{Learning}

In this final section we will focus on defining learning algorithms that converge to Nash equilibria in SGPGs and SPGs. In particular we will define a stochastic game version of log linear learning. We will show that this converges in SGPGs but not necessarily in SPGs. Finally we will introduce two variants that converge in SPGs.

\subsection{Log-linear Learning in Repeated Normal-form Games}

Log-linear learning is a learning algorithm that provably converges to pure Nash equilibria in potential games. That is, if players repeatedly play the same normal-form game, adjusting their strategies at each timestep according to the log-linear learning algorithm, their play will approach a pure Nash equilibrium in probability. In fact, one can say even more: log-linear learning will converge to {\em potential maximizing} pure Nash equilibria. This is referred to as an "equilibrium-selection property" in that the algorithm converges to a particular type of Nash equilibrium. In the context of potential games, the potential function often characterizes some notion of social/global utility, and so it is particularly useful to have an algorithm that will converge to such behavior.

Log-linear learning proceeds as follows. At each timestep $t$, players simultaneously select actions $a^i \in \AA^i$ and receive utility $u^i(a)$. The players select these actions according to probability distributions $x^i \in \AA^i$ over their action sets. The players update these distributions as follows. At time $t$, one player $i$ is selected uniformly at random and generates a distribution $x^i_t$. All other players will play the same action they did at the previous timestep, ie $a_{t}^{j} = a_{t-1}^{j}$ for players $j \neq i$. In contrast, player $i$ will play by sampling from the distribution

$$
x^i_t(a) = \dfrac{e^{u^i(a, a_{t-1}^{-i})}/\tau}{\sum_{b \in \AA^i} e^{u^i(b, a_{t-1}^{-i})}/\tau}
$$

where $a_{t-1}^{-i}$ is the list of actions played at time $t-1$ by players other than player $i$, $x^i_t(a)$ is the probability of selecting action $a$ under the distribution $x^i_t$, and $\tau$ is a {\em temperature parameter}. Notice that as the temperature parameter tends to zero, the probability distribution concentrates on best responses to the other players' actions at the last timestep. In [cite Blume statistical mechanics of strategic interaction] it is shown that the stationary distribution $\mu$ of joint strategies is

$$
\mu(a) = \dfrac{e^{\phi(a)/\tau}}{\sum_{b} e^{\phi(b)/\tau}}
$$

where $b$ varies over the set of joint actions $b \in \times_i \AA^i$.

From this explicit form of the stationary distribution it follows that as the temperature is brought to zero, the joint distribution concentrates on potential maximizing joint behaviors.


In [cite Marden revisiting log-linear learning, or maybe the paper they cite] the authors provide an alternative proof method using the theory of resistance trees in Markov processes. This allows the authors to characterize the stochastically stable states (which are potential maximizing joint actions), without explicitly writing down the stationary distribution.

*
*
This is a rewording of the review from "Revisiting log linear learning"

Let $P^0$ denote the probability transition matrix for a finite state Markov chain over the state space $Z$. We refer to $P^0$ as the unperturbed process. Consider a perturbed process where the size of the perturbation is indexed by a scalar $\epsilon > 0$, and let $P^{\epsilon}$ be the associated transition matrix. The process $P^{\epsilon}$ is called a regular perturbed Markov process if $P^{\epsilon}$ is ergodic for sufficiently small $\epsilon$ and $P^{\epsilon}$ approaches $P^0$ at an exponentially smooth rate. Specifically, the latter condition means that for all $z, z' \in Z$,

$$
\lim_{\epsilon \rightarrow 0^+} P^{\epsilon}_{z \rightarrow z'} = P^0_{z \rightarrow z'}
$$

and,

$$
P^{\epsilon}_{z \rightarrow z'} > 0 \mbox{ for some } \epsilon > 0 \mbox{ implies } \lim_{\epsilon \rightarrow 0^+} \dfrac{P^{\epsilon}_{z \rightarrow z'}}{\epsilon^{R(z \rightarrow z'}}
$$

for some nonnegative real number $R(z \rightarrow z')$, which we call the {\em resistance} of the transition $z \rightarrow z'$ under the perturbed process. 

Construct a complete directed graph with $|Z|$ vertices for each state. The vertex corresponding to state $z_j$ will be called $j$. The weight on the directed edge $i \rightarrow j$ is the resistance $R(z_i \rightarrow z_j)$. A $j$-tree, or ``tree rooted at $j$'', $T$, is a directed tree such that all paths terminate at $j$. The resistance of $T$ is the sum of the edge resistances composing it, and the {\em stochastic potential}, $\gamma_j$, of $z_j$ is the minimum resistance amongst all $j$-trees.

\begin{thm}[cite Evolution of convention]
Let $P^{\epsilon}$ be a regular perturbed Markov process, and for each $\epsilon > 0$ let $\mu^{\epsilon}$ be the unique stationary distribution of $P^{\epsilon}$. Then $\lim_{\epsilon \rightarrow 0^+} \mu_{\epsilon}$ exists and the limiting distribution $\mu^0$ is a stationary distribution of $P^0$. The stochastically stable states (i.e. the support of $\mu^0$) are precisely those states with minimal stochastic potential. Furthermore, if a state is stochastically stable then the state must be in a recurrent class of the unperturbed process $P^0$.
\end{thm}
 
*
*

In [Marden log-linear learning] the authors show that log-linear learning converges to potential maximizing behavior by proving the following sequence of results:

\begin{lem}[Marden 3.1]
Log-linear learning induces a regular perturbed Markov process where the unperturbed Markov process is an asynchronous best reply process and the resistance of any feasible transition $a \rightarrow b = (b^i, a^{-i})$ is
$$
R(a \rightarrow b) = \max_{a^i_* \in \AA^i} u^i(a^i_*, a^{-i}) - u^i(b)
$$
\end{lem}


\begin{lem}[Marden 3.2]
Consider any finite $n$-player potential game with potential function $\phi: \AA \rightarrow \R$ where all players adhere to log-linear learning. For any feasible action path
$$
\PP = \{a^0 \rightarrow a^1 \rightarrow \cdots \rightarrow a^m\}
$$

and its reverse path

$$
\PP^R = \{a^m \rightarrow a^{m-1} \rightarrow \cdots \rightarrow a^0\}
$$

the difference in the total resistance across the paths is

$$
R(\PP) - R(\PP^R) = \phi(a^0) - \phi(a^m)
$$
\end{lem}

and finally,

\begin{prop}[3.1 Marden]
Consider any finite $n$-player potential game with potential function $\phi: \AA \rightarrow \R$ where all players adhere to log-linear learning. The stochastically stable states are the set of potential maximizers, i.e., $\{ a \in \AA : \phi(a) = \max_{a^* \in \AA} \phi(a^*)  \}$
\end{prop}



\subsection{A Generalization to Stochastic Games}

This section will generalize log-linear learning to the stochastic setting and show that, in the case of stochastic global potential games, the stochastically stable states of generalized log-linear learning are precisely those joint behaviors that maximize the global potential function. 

Generalized log-linear learning proceeds as follows. At each timestep $t$, players select pure behaviors $\pi^i$. The players update their behavior choices as follows. At time $t$, a player $i$ and a state $s$ are selected uniformly at random. Player $i$ will generate a distribution $x^i_s$. All other players select the same behavior as they did at the previous timestep, i.e. $\pi_t^j = \pi_{t-1}^j$ for players $j \neq i$. In contrast, player $i$ will play by sampling from the distribution

\begin{comment}
$$
x^i_s(a) = \dfrac{e^{Q^i_{\pi}(s, a, a_{t-1}^{-i})}/\tau}{\sum_{b \in \AA^i} e^{Q^i_{\pi}(s, b, a_{t-1}^{-i})}/\tau}
$$

where $a_{t-1}^{-i}$ is the action played in state $s$ at time $t-1$ by all players except player $i$. $x^i_s(a)$ is the probability of selecting action $a$ in state $s$ under the distribution $x^i_s$, and $\tau$ is a {\em temperature parameter}. 

It is important to note that this learning method is not payoff-based. It requires that players are able to compute their $Q$-values in order to update their strategy.


\begin{lem}
Generalized log-linear is a regular perturbed Markov process where the resistance of any feasible transition $\pi \rightarrow \tau = (a^i_s, \pi^{-i})$ is
$$
R(\pi \rightarrow \tau) = \max_{a^i_* \in \AA^i} Q^i_{\pi}(a^i_*, \pi^{-i}) - Q^i_{\pi}(s, \tau)
$$
\end{lem}

\begin{proof}
The unperturbed process behaves as follows for each timestep $t > 0$. Choose a player $i$ and state $s$, both uniformly at random. Change player $i$'s action in that timestep to an element of $a_* \in \argmax Q^i_{\pi_{t-1}}(s, a, \pi^{-i}_{s})$. Keep the behavior of player $i$ in all other states unchanged, and keep the behaviors of all other players the same.

Under the perturbed process $P^{\epsilon}$, the probability of transitioning from $\pi$ to $\tau$ is

$$
P^{\epsilon}_{\pi \rightarrow \tau} = \dfrac{1}{n} \dfrac{1}{|\SS|} \dfrac{\epsilon^{-Q^i_{\pi}(s, a^i_s, \pi^{-i}_s})}{\sum_{b \in \AA^i} \epsilon^{-Q^i_{\pi}(s, b, \pi^{-i}_s})}
$$

Let $W_i(s, \pi) = \max_{a^i_* \in \AA^i} Q^i_{\pi}(a^i_*, \pi^{-i})$. Multiply the numerator and denominator by $\epsilon^{W_i(s, \pi)}$ yields

$$
P^{\epsilon}_{\pi \rightarrow \tau} = \dfrac{1}{n} \dfrac{1}{|\SS|} \dfrac{\epsilon^{W_i(s, \pi)-Q^i_{\pi}(s, a^i_s, \pi^{-i}_s})}{\sum_{b \in \AA^i} \epsilon^{W_i(s, \pi)-Q^i_{\pi}(s, b, \pi^{-i}_s})}
$$

So then,

$$
\lim_{\epsilon \rightarrow 0^+} \dfrac{P^{\epsilon}_{\pi \rightarrow \tau}}{\epsilon^{W_i(s, \pi)-Q^i_{\pi}(s, a^i_s, \pi^{-i}_s})} \\
= \lim_{\epsilon \rightarrow 0^+} \dfrac{1}{n |\SS|} \dfrac{1}{\sum_{b \in \AA^i} \epsilon^{W_i(s, \pi)-Q^i_{\pi}(s, b, \pi^{-i}_s})}
$$

The summands in the denominator will approach either 0 or 1 depending on if $b$ is in $\argmax Q^i_{\pi_{t-1}}(s, a, \pi^{-i}_{s})$. So this limit is equal to

$$
\dfrac{1}{n |\SS| |\argmax Q^i_{\pi_{t-1}}(s, a, \pi^{-i}_{s})|}
$$

Therefore generalized log-linear learning is a regular perturbed Markov process with resistances
$$
R(\pi \rightarrow \tau) = \max_{a^i_* \in \AA^i} Q^i_{\pi}(a^i_*, \pi^{-i}) - Q^i_{\pi}(s, \tau)
$$
\end{proof}

*Should define feasible paths here*


\begin{lem}
Consider any finite $n$-player SGPG with potential function $\Phi: \AA \rightarrow \R$ where all players adhere to log-linear learning. For any feasible action path
$$
\PP = \{\pi_0 \rightarrow \pi_1 \rightarrow \cdots \rightarrow \pi_m\}
$$

and its reverse path

$$
\PP^R = \{\pi_m \rightarrow \pi_{m-1} \rightarrow \cdots \rightarrow \pi_0\}
$$

the difference in the total resistance across the paths is

$$
R(\PP) - R(\PP^R) = \phi(\pi_0) - \phi(\pi_m)
$$
\end{lem}

\begin{proof}
Consider a single edge $\pi_k \rightarrow \pi_{k+1}$ in the path and its reverse $\pi_{k+1} \rightarrow \pi_k$. By the previous lemma we have

$$
R(\pi_k \rightarrow \pi_{k+1}) = W_i(s, \pi_k) - Q^i_{\pi_k}(s, a^i_{k+1}, (\pi^{-i}_k)_s) \\
R(\pi_{k+1} \rightarrow \pi_{k}) = W_i(s, \pi_{k+1}) - Q^i_{\pi_{k+1}}(s, a^i_{k}, (\pi^{-i}_{k+1})_s) 
$$

Since $\pi_k$ and $\pi_{k+1}$ only differ by player $i$'s action in state $s$ we have that:

$$
W_i(s, \pi_{k+1}) = W_i(s, \pi_{k}) \\
Q^i_{\pi_{k+1}}(\cdot) = Q^i_{\pi_{k}}(\cdot) \\
(\pi^{-i}_{k+1})_s = (\pi^{-i}_{k})_s
$$

So then

$$
R(\PP) - R(\PP^R) = Q^i_{\pi_k}(s, a_k^i, (\pi^{-i}_k)_s) - Q^i_{\pi_k}(s, a_{k+1}^i, (\pi^{-i}_k)_s) = \Phi(s, \pi_k) - \Phi(s, \pi_{k+1})
$$
\end{proof}
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

$$
x^i_s(a) = \dfrac{e^{V^i_{\asub{\pi_{t-1}}{a}}(s_0)/\tau}}{\sum_{b \in \AA^i} e^{V^i_{\asub{\pi_{t-1}}{b}}(s_0)/\tau} }
$$

where $\pi_{t-1}$ is the joint behavior played at time $t-1$ and $s_0$ is the initial state of the stochastic game. $x^i_s(a)$ is the probability of selecting action $a$ in state $s$ under the distribution $x^i_s$, and $\tau$ is a {\em temperature parameter}. 

It is important to note that this learning method is not payoff-based. It requires that players are able to compute their state values in order to update their strategy.


\begin{lem}
Generalized log-linear is a regular perturbed Markov process where the resistance of any feasible transition $\pi \rightarrow \tau = (a^i_s, \pi^{-i})$ is
$$
R(\pi \rightarrow \tau) = \max_{a^i_* \in \AA^i} V^i_{\asub{\pi}{a^i_*}}(s_0) - V^i_{\tau}(s_0)
$$
\end{lem}

\begin{proof}
The unperturbed process behaves as follows for each timestep $t > 0$. Choose a player $i$ and state $s$, both uniformly at random. Change player $i$'s action in that timestep to an element of $a_* \in \argmax_a {V^i_{\asub{\pi_{t-1}}{a}}(s_0)}$. Keep the behavior of player $i$ in all other states unchanged, and keep the behaviors of all other players the same.

Let $\epsilon = e^{-1/\tau}$. Under the perturbed process $P^{\epsilon}$, the probability of transitioning from $\pi$ to $\tau$ is

$$
P^{\epsilon}_{\pi \rightarrow \tau} = \dfrac{1}{n} \dfrac{1}{|\SS|} \dfrac{\epsilon^{-V^i_{\asub{\pi}{a^i_s}}(s_0)}}{\sum_{b \in \AA^i} \epsilon^{-V^i_{\asub{\pi}{b}}(s_0)}}
$$

Let $W_i(s, \pi) = \max_{a \in \AA^i} V^i_{\asub{\pi}{a}}(s_0)$. Multiply the numerator and denominator by $\epsilon^{W_i(s, \pi)}$ yields

$$
P^{\epsilon}_{\pi \rightarrow \tau} = \dfrac{1}{n} \dfrac{1}{|\SS|} \dfrac{\epsilon^{W_i(s, \pi)-V^i_{\asub{\pi}{a}}(s_0)}}{\sum_{b \in \AA^i} \epsilon^{W_i(s, \pi)-V^i_{\asub{\pi}{a}}(s_0)}}
$$

So then,

$$
\lim_{\epsilon \rightarrow 0^+} \dfrac{P^{\epsilon}_{\pi \rightarrow \tau}}{\epsilon^{W_i(s, \pi)-V^i_{\asub{\pi}{a}}(s_0)}} \\
= \lim_{\epsilon \rightarrow 0^+} \dfrac{1}{n |\SS|} \dfrac{1}{\sum_{b \in \AA^i} \epsilon^{W_i(s, \pi)-V^i_{\asub{\pi}{a}}(s_0)}}
$$

The summands in the denominator will approach either 0 or 1 depending on if $b$ is in $\argmax V^i_{\asub{\pi}{a}}(s_0)$. So this limit is equal to

$$
\dfrac{1}{n |\SS| |\argmax V^i_{\asub{pi}{a}}(s_0)|}
$$

Therefore generalized log-linear learning is a regular perturbed Markov process with resistances
$$
R(\pi \rightarrow \tau) = \max_{a^i_* \in \AA^i} V^i_{\asub{\pi}{a^i_*}}(s_0) - V^i_{\tau}(s_0)
$$
\end{proof}

In order to relate the graph-theoretic notion of stochastic potential to the stochastic game-theoretic notion of global potential we must define feasible paths and reverse paths. A feasible path is a sequence of joint behaviors $\PP = \{\pi_0 \rightarrow \pi_1 \rightarrow \cdots \rightarrow \pi_m\}$ such that two adjacent behaviors differ by the action of a single player in a single state. The resistance $R(\PP)$ of a path is the sum of edge resistances

$$
R(\PP) = \sum_{i=1}^n R(\pi_{i-1} \rightarrow \pi_i)
$$

The reverse path consists of the same joint behaviors, but with arrows travelling in the opposite direction. We can relate path resistances with changes in global potential with the following lemma.


\begin{lem}
Consider any finite $n$-player SGPG with potential function $\Phi: \AA \rightarrow \R$ where all players adhere to log-linear learning. For any feasible action path
$$
\PP = \{\pi_0 \rightarrow \pi_1 \rightarrow \cdots \rightarrow \pi_m\}
$$

and its reverse path

$$
\PP^R = \{\pi_m \rightarrow \pi_{m-1} \rightarrow \cdots \rightarrow \pi_0\}
$$

the difference in the total resistance across the paths is

$$
R(\PP) - R(\PP^R) = \phi(s_0, \pi_0) - \phi(s_0, \pi_m)
$$
\end{lem}

\begin{proof}
Consider a single edge $\pi_k \rightarrow \pi_{k+1}$ in the path and its reverse $\pi_{k+1} \rightarrow \pi_k$. By the previous lemma we have

$$
R(\pi_k \rightarrow \pi_{k+1}) = W_i(s, \pi_k) - V^i_{\pi_{k+1}}(s_0) \\
R(\pi_{k+1} \rightarrow \pi_{k}) = W_i(s, \pi_{k+1}) - V^i_{\pi_k}(s_0)
$$

Since $\pi_k$ and $\pi_{k+1}$ only differ by player $i$'s action in state $s$ we have that:

$$
W_i(s, \pi_{k+1}) = W_i(s, \pi_{k}) \\
$$

So then

$$
R(\PP) - R(\PP^R) = V^i_{\pi_k}(s_0) - V^i_{\pi_{k+1}}(s_0) = \Phi(s_0, \pi_k) - \Phi(s_0, \pi_{k+1})
$$

Adding these differences up over all pairs in the path gives the desired result.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{prop}
Consider any finite $n$-player GSPG with potential function $\Phi: \AA \rightarrow \R$ where all players adhere to log-linear learning. The stochastically stable states are the set of potential maximizers, i.e., $\{ \pi : \Phi(\pi) = \max_{\pi_*} \Phi(\pi_*)  \}$
\end{prop}

\begin{proof}
Let $\pi$ be a behavior with minimum stochastic potential, and let $G$ be a $\pi$-tree that realizes this stochastic potential. Let $\pi_*$ be a behavior that minimizes $\Phi(s_0, \cdot)$.

Within the graph $G$ there is a path $\PP$ beginning at $\pi_*$ and ending at $\pi$. Let $G_*$ be the graph obtained by removing $\PP$ from $G$ and replacing it with $\PP^R$. $G_*$ is a tree rooted at $\pi_*$. By the previous lemma,

$$
R(G) - R(G_*) = \Phi(s_0, \pi) - \Phi(s_0, \pi_*)
$$

Since $\pi$ has minimum stochastic potential, the right hand side of the equation is greater than or equal to zero. Hence

$$
\Phi(s_0, \pi) \leq \Phi(s_0, \pi_*)
$$

and so $\pi$ must also minimize $\Phi(s_0, *)$.
\end{proof}



\subsection{Generalized Log-Linear Learning does not converge in Stochastic Potential Games}

The first question we need to answer when addressing log-linear learning for SPGs is one of goals. In repeated potential games, log-linear learning is attractive as a method because it guarantees potential maximizing behavior. Potential maximizers are a subset of pure Nash equilibria in potential games, and therefore log-linear learning exhibits an "equilibrium selection" property, in that it selects Nash equilibria with an additional restriction (namely those that maximize potential). When we extended log-linear learning to SGPGs there was still a clear goal: define a learning algorithm whose stationary distribution is supported on Nash equilibria that maximize the global potential function. Generalized log-linear learning selects for these more restrictive Nash equilibria in SGPGs. In contrast, an SPG does not necessarily have a global potential function, and so first we need to describe the kind of equilibria we would like log-linear learning to select for in SPGs. The kind of equilibria we are interested in are {\em simultaneously potential maximizing}.

\begin{mydef}
Let $\Gamma$ be an SPG. A joint behavior $\pi$ is simultaneously potential maximizing if for all states $s$, the joint strategy $\pi_s$ is a potential maximizing strategy in the continuation stage game $G_s(V_{\pi})$. 
\end{mydef}

Recall that for an SPG $\Gamma$, any continuation game $G_s(c)$ is a potential game. As a result we are able to talk about potential maximizing strategies for $G_s(V_{\pi})$. 


We've remedied one issue that the lack of global potential presents us. However, there is a deeper issue: without a global potential function we do not have a global way to characterize resistances in log-linear learning for SPGs. As a result, a "naive" attempt to use log-linear learning in SPGs will fail. We will illustrate this with an example of an SPG that does not converge under a naive log-linear learning rule. Like all of our previous discussions of SPGs, we will be in the finite-time setting.

{\bf Naive log-linear learning}. At each timestep $t$ all players select actions $a^i_s(t)$ in all states simultaneously. Let $a_s(t)$ denote the joint action taken in state $s$ at time $t$, and $a(t)$ denote the entire behavior at time $t$. At time $t$, a player $i$ and state $s$ are selected uniformly at random. Player $i$ selects an action according to the distribution

$$
pr(a^i) = \dfrac{e^{Q^i_{a(t-1)}(s, a^i, a^{-i}_s(t-1))/\tau}}{\sum_{b^i}e^{Q^i_{a(t-1)}(s, b^i, a^{-i}_s(t-1))/\tau}}
$$

where $\tau$ is a temperature parameter. The action taken in all other states is the same as at time $t-1$, and the action taken by all other players in state $s$ is the same as at time $t-1$. We are interested in understanding the stochastically stable states of this process. Namely we'd like to answer

\begin{enumerate}
\item In an SPG, are the stochastically stable states of this process simultaneously potential maximizing Nash equilibria?
\item In an SPG, are the stochastically stable states of this process Nash equilibria?
\end{enumerate}

The following example will show that neither of these statements are necessarily true.


\begin{eg}
Consider the 3 state, 2-horizon game $\Gamma$ described as follows. $\Gamma$ is a two player game, and in each state both players have two actions $a$ and $b$. Every episode begins in state $s_1$ and transitions either to state $s_2$ or $s_3$ where the players take a final move and the episode ends. The stage games for $s_1$ and $s_3$ are trivial (that is, all utilities are zero) and the stage game for $s_2$ is defined by the bimatrix:

\[
\begin{blockarray}{ccc}
 & a & b \\
\begin{block}{c(cc)}
  a & 100,100 & -100,100 \\
  b & 100,-100 & -100,-100 \\
\end{block}
\end{blockarray}
 \]

Notice that by construction, all stage games are potential games with trivial potential functions $\phi \equiv 0$. The transition matrices are

$$
P_{12} = 
\begin{pmatrix} 
0 & 0.5 \\
0.5 & 1 
\end{pmatrix}
$$

and

$$
P_{13} = 
\begin{pmatrix} 
1 & 0.5 \\
0.5 & 0. 
\end{pmatrix}
$$

One can describe the pure Nash equilibria of this game as follows. Both players may act arbitrarily in state $s_3$ and $s_2$ (since all actions are potential maximizing in those states). However, the action to be taken in state $s_1$ is determined based on the action in $s_2$. The equilibria are:

\[
\begin{blockarray}{ccc}
 s_1 & s_2 & s_3 \\
\begin{block}{ccc}
  (a,a) & (a,a) & (\cdot, \cdot) \\
  (a,b) & (a,b) & (\cdot, \cdot) \\
  (b,a) & (b,a) & (\cdot, \cdot) \\
  (b,b) & (b,b) & (\cdot, \cdot) \\
\end{block}
\end{blockarray}
 \]
 
Where $(\cdot, \cdot)$ denotes that the players make perform any actions in $s_3$. 

Now, suppose players adhere to log-linear learning as described above. Our goal is to show that the unperturbed process may move the behavior out of Nash equilibrium. Suppose at time $t$ the joint action is
$$
a^1(t) = [a, a, a]
a^2(t) = [a, a, a]
$$

With positive probability, the following will occur at time t:
\begin{itemize}
\item state 2 is selected
\item player 2 is selected
\item player 2 changes their action from $a$ to $b$
\end{itemize}

But then the new joint action is
$$
a^1(t) = [a, a, a]
a^2(t) = [a, b, a]
$$

which does not simultaneously maximize potentials, and is not a Nash equilibrium.

\end{eg}

The problem that this example brings forward is that an action change in a layer $k$ state may alter the continuation games played in layer $l < k$ states such that their behaviors are no longer Nash equilibria. This is exactly what happens in our example - asynchronous best response dynamics can alter the behavior in state $s_2$, which in turn changes the continuation game played at $s_1$ and what constitutes Nash equilibrium.



\subsection{Log-Linear Learning with Inertia}

To remedy the problem introduced in the last section, we first introduce a modified version of log-linear learning for {\em repeated} potential games. We call this modfied version "log-linear learning with inertia". The idea of log-linear learning with inertia is to force players to eventually settle their behavior to a fixed pure Nash equilibrium. This will later allow us to build log-linear learning with inertia into an SPG learning algorithm. 

Let $G$ be a potential game. log-linear learning with inertia proceeds as follows.

{\bf log-linear learning with inertia algorithm} We describe the parametrized perturbed processes of log-linear learning with inertia $P^{\epsilon}$. At time $t$ a player $i$ is selected uniformly at random. Their behavior is updated as follows:
\begin{enumerate}
\item If $a^i(t-1)$ was a best response to $a^{-i}(t-1)$ then with probability $1-\epsilon$ set $a^i(t) = a^i(t-1)$ (we will refer to this as the inertia of the process)
\item Otherwise, select $a^i(t)$ according to the probability distribution
$$
p(a^i) = \dfrac{\epsilon^{-u^i(a^i, a^{-i}(t-1))}}{\sum_{b^i} \epsilon^{-u^i(b^i, a^{-i}(t-1)}}
$$
\end{enumerate}

We'll refer to the unperturbed process $P^0$ as "asynchronous best response with inertia". In this process, at time $t$ a player $i$ is selected uniformly at random. If $a^i(t-1)$ was a best response to $a^{-i}(t-1)$ then player $i$ repeats that action. Otherwise, they select a best response to $a^{-i}(t-1)$. We would like to show that the stochastically stable states of log-linear learning with inertia are potential maximizing behaviors, and that under log-linear learning with inertia behavior converges to a fixed pure Nash equilibrium. We begin by showing that log-linear learning indeed defines a regular perturbed Markov process with the unperturbed process being asynchronous best response with inertia.

\begin{lem}
Log-linear learning with inertia induces a regular perturbed Markov process where the unperturbed Markov process is an asynchronous best response process with inertia and the resistance of any feasible transition $a^0 \rightarrow a^1$ is 
$$
R(a^0 \rightarrow a^1) = max_{a^i_* \in \AA^i} u^i(a^i_*, a^{-i}_0) - u^i(a_1)
$$

if $a^i_0$ is not a best response to $a^{-i}_0$ and

$$
R(a^0 \rightarrow a^1) = max_{a^i_* \in \AA^i} u^i(a^i_*, a^{-i}_0) - u^i(a_1) + 1
$$

if $a^i_0$ is a best response to $a^{-i}_0$.

\end{lem}
\begin{proof}
It is straightforward to see that $P^{\epsilon}$ defines a finite aperiodic and irreducible process over the joint action space. So, we only need to show that $P^{\epsilon}$ approaches $P^0$ at an exponentially smooth rate and compute the associated resistances.

Let $a_0 \rightarrow a_1$ be a feasible transition which means that $a_1 = (a^i_1, a_0)$. That is, $a_1$ only differs from $a_0$ in a single players action.

First, suppose that $a^i_0$ is not a best response to $a^{-i}_0$. Then

$$
P^{\epsilon}_{a_0 \rightarrow a_1} = \dfrac{1}{n} \dfrac{\epsilon^{-u^i(a_1)}}{\sum_{b^i} \epsilon^{-u^i(b^i, a^{-i}_0}}
$$

Let $V^i(a_0^{-i}) = max_{a^i_* \in \AA^i} u^i(a^i_*, a^{-i}_0)$. Then

\begin{align*}
\lim_{\epsilon \rightarrow 0} \dfrac{P^{\epsilon}_{a_0 \rightarrow a_1}}{\epsilon^{V^i(a_0^{-i}) - u^i(a_1)}} \\
= \lim_{\epsilon \rightarrow 0} \dfrac{1}{n} \dfrac{\epsilon^{-V^i(a_0^{-i})}}{\sum_{b^i} \epsilon^{-u^i(b^i, a^{-i}_0}} \\
= \lim_{\epsilon \rightarrow 0} \dfrac{1}{n} \dfrac{1}{\sum_{b^i} \epsilon^{V^i(a_0^{-i})-u^i(b^i, a^{-i}_0}}
\end{align*}

and $\lim_{epsilon \rightarrow 0} \epsilon^{V^i(a_0^{-i})-u^i(b^i, a^{-i}_0}$ is an indicator that is supported on the best responses of $a^{-i}_0$. Therefore,

$$
\lim_{\epsilon \rightarrow 0} \dfrac{P^{\epsilon}_{a_0 \rightarrow a_1}}{\epsilon^{V^i(a_0^{-i}) - u^i(a_1)}} = \dfrac{1}{n |BR(a^{-i}_0|}
$$

and so when $a^i_0$ is not a best response to $a^{-i}_0$, the resistance of the transition $a_0 \rightarrow a_1$ is $V^i(a_0^{-i}) - u^i(a_1)$.

Now suppose that $a^i_0$ is a best response to $a^{-i}_0$. Then

$$
P^{\epsilon}_{a_0 \rightarrow a_1} = \dfrac{\epsilon}{n} \dfrac{\epsilon^{-u^i(a_1)}}{\sum_{b^i} \epsilon^{-u^i(b^i, a^{-i}_0}}
$$

The additional $\epsilon$ terms comes from the fact that, if player $i$ is selected, they will keep action $a^i_0$ with probability $1-\epsilon$ due to inertia. This expression differs from [labelled earlier transition probability] by a factor of $\epsilon$, and hence the resistance of this transition will be one larger:

$$
R(a_0 \rightarrow a_1) = V^i(a_0^{-i}) - u^i(a_1) + 1
$$
\end{proof}


Now that we have a formula for the resistance in terms of player utilities, the next step is to show that the actions with minimal stochastic potential are potential maximizers (when the underlying game is a potential game). 

\begin{lem}
Let $G$ be a potential game with potential function $\phi$ and let $\PP = \{a_0 \rightarrow \cdots \rightarrow a_m\}$ be a feasible path such that $a_m$ maximizes $\phi$. Then $R(\PP^R) - R(\PP)$ is positive.
\end{lem}

\begin{proof}

\end{proof}

\subsection{Generalized Log-Linear Learning with Inertia in Stochastic Potential Games}






% Feasible paths only travel backwards through layers. As long as behavior eventually settles to a particular equilibrium the learning algorithm should work. Within a layer, we perform learning until we're very close to a potential maximizer. have to bound how much the potential changes at last state. 

















