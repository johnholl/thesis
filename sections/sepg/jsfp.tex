
\section{Joint Strategy Fictitious play with Inertial in Stochastic Games}


\subsection{Background on JSFP for repeated games}


In standard Fictitious play (FP), agents learn by selecting best responses to the empirical past play of their opponents. If $a_t$ is the joint action played at stage $t$, then $a_{t+1}^i$ is obtained as a best response

$$
a_{t+1}^i \in BR(x_t^{-i})
$$


to the (likely mixed) opponent actions $x_t^j$ where 

$$
x_t^j = \frac{1}{t} \sum_{k=1}^t a_k^j
$$

Here we identify an action $a_k^i$ with the discrete probability distribution with all probability concentrated on $a_k^i$. A different way of describing the choice of $a_{t+1}^i$ is that it is an action which maximizes $u^i(\cdot, x_t^{-i})$ where $u^i$ is the utility function of player $i$.



The essential difference between FP and Joint strategy fictitious play (JSFP) is that, from the perspective of player $i$, JSFP treats all other opponents as if they were a single entity. This manifests in the way that player $i$ calculates empirical frequencies of opponent play. JSFP will calculate $a_{t+1}^i$ as a best response

$$
a_{t+1}^i \in BR(z_t^{-i})
$$

to the empirical joint opponent play

$$
z_t^{-i} = \frac{1}{t} \sum_{k=1}^t a_{k}^{-i}
$$

where we identify the joint opponent action $a_{k}^{-i}$ with the probability distribution over joint opponent actions with all probability concentrated on $a_{k}^{-i}$. As in FP, the action choice $a_{t+1}^i$ can also be viewed as maximizing $u^i(\cdot, z_t^{-i})$. On initial inspection, it seems that JSFP has a very costly memory constraint compared to FP since each player needs to maintain empirical joint strategy counts. However, it is in fact not necessary for players to keep track of $z_t^{-i}$, but instead each player can instead track and maximize $u^i(\cdot, z_t^{-i})$ directly. By linearity,
$$
u^i(y, z^{-i}_t) = \frac{1}{t} \sum_{k=1}^t u^i(y, a^{-i}_t)
$$ 

At each timestep, this quantity can be updated with the following recursive formula:

$$
u^i(y, z^{-i}_{t+1}) = \frac{1}{t+1} u^i(y, a^{-i}_{t+1}) + \frac{t}{t+1} u^i(y, z^{-i}_t)
$$

At time $t$, player $i$ can now calculate $\text{argmax}_{y \in \AA} u^i(y, z^{-i}_t)$ and therefore can perform JSFP without maintaining the empirical joint distribution.


{\bf JSFP with Inertia} [Marden paper] modify JSFP with an inertial condition to guarantee convergence. In this paragraph we will describe JSFP with Inertia and then state the main theorem of [Marden paper]. At time $t$ player $i$ will play according to the following rule:
\begin{enumerate}
\item If $a_{t-1}^i$ is a best response to $z^{-i}_t$, then $a_t^i = a_{t-1}^i$. That is, player $i$ repeats their action.
\item Otherwise, player $i$ will repeat their action with probability $\alpha^i(t)$ or will select a different action according to the probability distribution $\beta_i(t)$ with probability $1-\alpha^i_t$. $\beta_i(t)$ can be any distribution with full support. We assume that $\alpha^i(t)$ is bounded away from $0$ and $1$.
\end{enumerate}

\begin{mydef}
A player is indifferent between two strategies $a_1$ and $a_2$ if there exists a joint opponent play $a^{-i}$ such that
$$
u^i(a_1, a^{-i}) = u^i(a_2, a^{-i})
$$

Note that the player actions do not have to yield the same utilities for all opponent plays; one will suffice.
\end{mydef}


\begin{thm}[Marden]
In any finite generalized ordinal potential game in which no players are indifferent between two strategies the joint actions $a_t$ generated by JSFP with inertia will converge to a pure Nash equilibrium almost surely.
\end{thm}


\subsection{Extension to Finite-time SPGs}


**
The almost-sure nature of convergence allows us to make a straightforward extension of the JSFP with inertia algorithm to the finite-time SPG setting. This establishes that there exists a decoupled learning method that converges to a pure Nash equilibrium in SPGs.

For an SPG $\Gamma$ our learning setting is as follows. At each time $t$, each player $i$ must choose an action $a^i_s(t)$ for every state $s \in \SS$. We let $z_s(t)$ denote the empirical joint play in state $s$ at time $t$, and $z^{-i}_s(t)$ denote the empirical joint play of opponents against player $i$ in state $s$ at time $t$. We begin by defining an extension of JSFP to stochastic games and after this define the full algorithm which employs inertia.

{\bf Stochastic Game JSFP (SG-JSFP)} At each timestep, players will update two state-dependent quantities: their behavior $a^i_s(t)$ and continuation payoffs $c^i_s(t)$. At time $t$, player $i$ computes a best response $a^i_s(t+1)$ to $z_s^{-i}$ in the game  $G_s(c^i(t))$ which has utilities
$$
u^i_s(a) + \sum_{s' \in \SS} P_{ss'}(a) c^i_{s'}(t)
$$

For ease of notation we will simply refer to $G_s(c^i(t))$ as $G_s(t)$. Once the actions $a_s(t+1)$ are computed, the continuation payoff is updated to be the utility player $i$ receives in game $G_s(t)$ when joint action $a_s(t+1)$ is taken. Explicitly:

$$
c^i_s(t+1) = u^i_s(a_s(t+1)) + \sum_{s' \in \SS} P_{ss'}(a_s(t+1)) c^i_{s'}(t)
$$


As in JSFP, players can perform SG-JSFP without explicitly maintaining the empirical joint play $z_s(t)$ in every state. At each timestep player $i$ must select an action which is a best response to $z^{-i}_s(t)$ in game $G_s(t)$. In the exact same way as JSFP, player $i$ can update their expected immediate utility for playing action $y$ against the empirical joint opponent play via

$$
u^i_s(y, z^{-i}_s(t+1)) = \frac{1}{t+1} u^i_s(y, a^{-i}_s(t+1)) + \frac{t}{t+1}u^i_s(y, z^{-i}_s(t))
$$

And then player $i$ can calculate the utility in game $G_s(t)$ of any action $y$ according to

$$
u^i_s(y, z^{-i}_s(t+1)) + \sum_{s' \in \SS} P_{ss'}(a) c^i_{s'}(t+1)
$$

Note that this requires the player to know the "structure of the game". In standard JSFP, knowing the structure of the game simply means that a player can calculate any value of their utility function. In the stochastic game setting this knowledge requirement is stronger - players must also be able to calculate transition probabilities $P_{ss'}(a)$. 



{\bf SG-JSFP with inertia} We now introduce SG-JSFP with inertia. At each timestep player $i$ must select an action in each state. We wish to impose additional decision-making rules on top of SG-JSFP which guarantee convergence to pure Nash equilibrium. They are:

\begin{enumerate}
\item If $a_s^i(t-1)$ is a best response to $z^{-i}_t$ in the game $G_s(t)$ then $a^i_s(t) = a^i_s(t-1)$.
\item Otherwise, player $i$ will repeat their action in state $s$ with probability $\alpha^i(t)$ or will select a different action according to the probability distribution $\beta_i(t)$ with probability $1 - \alpha^i(t)$. $\beta_i(t)$ can be any distribution with full support and $\alpha^i(t)$ must be bounded away from $0$ and $1$.
\end{enumerate}

The last thing we must address before the theorem is "indifference to distinct strategies". This is needed to show that JSFP with inertia converges to a pure Nash equilibrium in (repeated) potential games. We will need a similar kind of indifference to guarantee our theorem. 

\begin{mydef}
We say that a stage game $G_s(c)$ with continuation payoff $c$ is indifferent to distinct strategies of one can find two joint plays $a_1$ and $a_2$ differing only in player $i$'s behavior such that the utility of player $i$ is the same. That is,
$$
u^i_s(a_1) + \sum_{s'} P_{ss'}(a_1)c^i_{s'} = u^i_s(a_1) + \sum_{s'} P_{ss'}(a_1)c^i_{s'}
$$
\end{mydef}

\begin{mydef}
We say that a continuation payoff $c$ (indexed by states and players) is "grounded" or "grounded in reality" if it is equal to a value function of some joint behavior. That is, there exists a joint behavior $\pi$ such that
$$
c^i_s = V^i_{\pi}(s)
$$
\end{mydef}

\begin{mydef}
We say that a stochastic game $\Gamma$ is "indifferent to distinct strategies of grounded continuations" if $G_s(c)$ is indifferent to distinct strategies whenever $c$ is grounded in reality.
\end{mydef}

\begin{thm}
In any finite-time SPG that is indifferent to distinct strategies of grounded continuations, the behaviors $a^i(t)$ converge to a pure Nash equilibrium almost surely.
\end{thm}

\begin{proof}

This proof relies on two very simple facts. First, JSFP with inertia converges to a pure Nash equilibrium in potential games. Second, for any continuation payoff $c$, the game $G_s(c)$ is a potential game.

To show almost sure convergence, we can show that with probability one there is a finite time $T$ such that $a_T$ consititutes a pure Nash equilibrium in the stochastic game. Once such an action is played, the inertial condition (1) guarantees that it will continue to be played forever.

Let $L$ be the number of layers/times that elapse in the game before an episode ends. Consider states $s$ that arise in the final layer/time of the game. The games played in these states are each potential games, and are never modified with a continuation payoff since the episode ends after an action is taken in any of these states. Hence $G_s(t) = G_s$ for all times. It is clear that SG-JSFP with inertia reduces simply to JSFP with inertia in these states, and therefore by [Marden], with probability one there is a finite time $T_1(s)$ for each state in this layer such that the action profiles constitute a Nash equlibrium. By the inertial condition (1), since the stage game $G_s(t)$ is not changing, players will play the same action in state $s$ forever once it is played once. Since the number of states is finite, this means that with probability one there is a time $T_1$ where all action profiles in all layer $L$ states are simultaneously Nash equilibria.

Now consider layer $L-1$ states. Initially, the stage games $G_s(t)$ are changing as the payoffs in layer $L$ change. However, after time $T_1$, these continuation payoffs stabilize to a constant value, and therefore $G_s(t) = G_s(t')$ for times $t, t' > T_1$. As a result, eventually SG-JSFP with inertia is identical with JSFP with inertia in the repeated games $G_s(T_1)$. Hence, since we are in an SPG and all continuation games are potential games, behavior in each layer $L-1$ state $s$ converges to a pure Nash equilibrium of $G_s(T_1)$ with probability one in finite time $T_2(s)$. Again, since the number of states is finite, with probability one there is a time $T_2$ such that all layer $L-1$ states are playing Nash equilibria to their continuation games.

We may repeat this argument moving backwards step-by-step through all layers of the stochastic game and conclude that with probability one, there is a finite time $T_L$ such that for all $t > T_L$ and states $s$, $a_s(t)$ is a Nash equilibrium for the game $G_s(T_L)$. Furthermore, $G_s(T_L)$ is the "true" continuation game. That is, the continuation payoff of player $i$ for the game $G_s(T_L)$ is $V^i_{a}(s)$. Therefore $a_t$ is a pure Nash equilibrium for the stochastic game.



\end{proof}


\subsection{SG-JSFP with inertia for SGPGs}

Let's return for a moment to the proof of [previous theorem] and consider the learning process in an intermediate layer $k$. In terms of learning, nothing that happens in the first $T_{L-k}$ steps has any bearing on what behaviors eventually converge to in this layer. In particular, only two facts were important when establishing convergence to pure Nash equilibrium in this layer:

\begin{enumerate}
\item Eventually the behavior in layer $k+1$ converged to a fixed pure Nash equilibrium, and therefore the continuation payoffs $c(t)$ also converged.

\item The final continuation games $G_s(t) = G_s(c(T_{L-k}))$ are potential games
\end{enumerate}


In order to guarantee the second point, we assumed that our stochastic game was an SPG. This guarantees that {\em any} continuation game is a potential game, and in particular, $G_s(c(T_1))$ is a potential game. However, these final continuation games are special: by construction their continuation payoff is grounded in reality. Namely,
$$
c^i_s(T_{L-k}) = V^i_{\pi}(s)
$$

where $\pi$ is a joint pure behavior where $\pi^i_s = a^i_s(T_{L-k})$ when $s$ is in a layer greater than $k$, and $\pi^i_s$ is arbitrary for any earlier layers. 

But, as we showed in section [section on SPGs and SGPGs], such continuation games will be potential games for SGPGs. Hence, the argument for SPGs will also work for SGPGs and we can conclude the following theorem:

\begin{thm}
In any finite-time SGPG that is indifferent to distinct strategies of grounded continuations, the behaviors $a^i(t)$ converge to a pure Nash equilibrium almost surely.
\end{thm}

\subsection{Conclusion on SG-JSFP with inertia}

The benefits of SG-JSFP is that it offers a convergent decoupled learning algorithm that converges to a pure Nash equilibrium in both SPGs and SGPGs. Furthermore, this algorithm has a small memory requirement which is similar to the memory requirement in regular JSFP (with inertia). One significant drawback of SG-JSFP with inertia is that it requires players to know the "structure of the game", in that they must be able to compute arbitrary stage game utility functions {\em and} transition probabilities. 













