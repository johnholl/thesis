\section{Learning Methods and Learning by Reinforcement}

The learning methods we presented in Chapter 3 were somewhat stylized and had significant knowledge requirements for players. For example, we made the assumption in SG-JSFP that all players select all actions in all states simultaneously. For both SG-JSPF and SG-LLL we also assumed that players could calculate their utility functions and that all players could observe the actions of all other players at every timestep. The first assumption for SG-JSFP can be dispelled with, but the second two assumptions are necessary for the implementation of both algorithms. 

Are there learning methods for SPGs and/or SGPGs that do not require players to compute their utility functions? These would resemble reinforcement-style algorithms, where players merely {\em estimate} their stage game utility functions from experience. Regret matching \cite{sergiu2013simple} is a learning method in iterated games that does not require computing utility functions, however its convergence guarantees are weaker than the learning methods we've discussed. In particular, strategies converge to the {\em set} of {\em coarse correlated equilibria}. So, one concrete research direction is to explore if regret matching can be extended to stochastic games, and if so, whether one can make universal convergence guarantees. This may not be straightforward - recall that we were unable to extend LLL to SPGs because the set of stochastically stable states contained more than one strategy. For the same reason it may be difficult to extend regret matching algorithms to stochastic games.