\section{Infinite Horizion SPGs}

It was important in our analyses of SPGs and SGPGs that we make them finite fixed-time. In particular this mean that states were visited only once in an episode. For SPGs we made use of this fact in arguing for both the existence of simultaneously potential maximizing equilibria, and the convergence of SG-JSFP to a pure Nash equilibrium. To briefly summarize its use: At a state in layer $k$, modification of behavior in this state can only change the continuation payoffs in layers less than $k$. So we were able to make arguments that iterate backwards through time, and be sure that once behavior stabilizes in layers $l > k$, the continuation payoffs for these layers will also remain constant.

Do simultaneously potential maximizing equilibria exist in SPGs? Are there even pure Nash equilibria? And are there convergent learning algorithms? These questions are not addressed by the current work. The author suspects however that these could be approached through the average reward formulation of MDPs.
