\section{Goal}
\label{sec:goal}

This work does not appear in chronological order. Chapter 4 corresponds to a project conducted in 2017. The work in Chapters 2 and 3 was done in 2018 and 2019. Chapter 4 conducts an empirical study of {\em deep reinforcement learning} (DeepRL) methods applied to multi-driver vehicle assignment and repositioning. DeepRL is a research area that combines {\em deep learning} and {\em reinforcement learning}. Deep learning consists of designing neural network architectures and training them with (stochastic) gradient descent and reinforcement learning is a learning paradigm through which one can train an agent to behave optimally in an environment. Chapters 2 and 3 can be viewed either as belonging to multi-agent systems or game theoretic control, depending on the reader's affinity. Chapter 2 examines two extensions of potential games to stochastic games. Potential games are known to admit a number of learning algorithms that converge to Nash equilibrium. Chapter 3 studies learning algorithms in the newly introduced classes of stochastic games. The next few paragraphs attempt to explain how the research in this work unfolded chronologically.

By 2017 DeepRL had seen a number of enormous successes. In 2014 researchers at Deepmind trained a DeepRL agent to play many Atari 2600 games at a superhuman level \cite{mnih2015human}. About one year later, Deepmind researchers again shocked the machine learning community when their computer Go player, dubbed "AlphaGo" \cite{silver2016mastering}, beat 9-dan ranked Lee Sedol in four out of five matches. The victories themselves were amazing, but were made even more incredible by the design of AlphaGo. The system was not given any dictionaries of opening moves, endgame sequences, or piece valuations. Rather, it was trained only by playing matches and learning "from scratch". That work has been carried forward and produced a new agent, "AlphaZero" \cite{alphazero}, that plays chess, shogi, and Go at superhuman levels and can be trained in about a day.

Despite striking empirical success, there is a notable lack of theoretical results in DeepRL. Reinforcement learning on its own provides methods for learning in an environment, but many of its celebrated results only have convergence guarantees in a tabular learning setting. Tabular methods are appropriate in situations where the state and action spaces are small, so that individual states and actions may be visited and tried many times. Deep learning vastly extends the applicability of reinforcement learning by incorporating function approximation, however this comes at the price of convergence guarantees. And yet, despite the dearth of theoretical results, DeepRL has had numerous successes and continues to be an area of great interest both in academia and industry.

While the empirical progress of DeepRL has outrun reinforcement learning theory, the tabular theory of reinforcement learning developed in the 1980's and 1990's \cite{watkins1992q, sutton2018reinforcement} was paramount to these modern triumphs. Deep Q-networks (DQN) \cite{mnih2015human}, deep deterministic policy gradient (DDPG) \cite{ddpg}, and trust region policy optimization (TRPO) \cite{trpo} are all examples of DeepRL algorithms whose designs are informed by tabular counterparts that preceded them by decades. Our present work is informed by the understanding that modern DeepRL owes its success to solid theoretical work in reinforcement learning. Many researchers have turned their focus towards integrating DeepRL algorithms in strategic, multi-agent interactions \cite{vinyals2017starcraft}. Yet, unlike single-agent reinforcement learning, the theory of tabular learning in the multi-agent settings is not well understood. Certain negative results are known, for example, that agents independently engaging in tabular Q-learning do not necessarily reach Nash equilibrium, even when the game has only one state [cite]. 


Chapter 4 presents empirical work in which DeepRL methods are applied to driver dispatching and repositioning problems at DiDi Taxi, a ridesharing company. Every few seconds, DiDi must take stock of available drivers in an area, requesting customer orders, and decide which drivers to dispatch to which orders. With an appropriately selected neural network architecture one may approach the problem with DeepRL techniques. One may either cast the problem as a single-agent problem in which the agent is DiDi, or a multi-agent problem in which the agents are drivers. Over the course of experimentation it became clear that while the single-agent learning method could sometimes outperform the multi-agent method in final performance, the multi-agent methods trained much faster, and generally was more stable. Specifically, the single-agent algorithm required more and more training time as the number of drivers increased, while the multi-agent training time was essentially constant. This suggests that decomposing a large single-agent problem into multiple smaller single-agent problems can be a powerful technique. However, from a theoretical perspective, the large single-agent problem enjoys convergence guarantees in a tabular setting while the multi-agent approach does not. 

This gap in empirical success and lack of theoretical justification motivated the work in Chapters 2 and 3. Much work in game theory and multi-agent reinforcement learning has explored learning in normal form games. The types of games in which players may learn to play Nash equilibria through uncoupled dynamics (which closely resemble reinforcement learning) remains an open area of research. However, a number of such games have been identified: zero sum games, team games, potential games, and supermodular games \cite{hofbauer2002global}. Very few games are known to converge to Nash equilibria under Q-learning. As far as that author is aware, the only positive results known are for zero sum games and 2-player partnership games \cite{leslie2005individual}. 

The DiDi work, as well as most domains of interest in reinforcement learning, involve problems where there is a state that evolves in time as a result of the decisions of an agent. Normal form games on the other hand are stateless. The appropriate extension of game theory to problems with state are known as stochastic games. Unlike learning in normal form games, learning in stochastic games is still quite fledgling. Very few subclasses of games that are amenable to learning methods have been identified in the literature. Some pioneering work has been done in the case of stochastic zero-sum games \cite{zerosumstochastic} and stochastic team games \cite{arslan2016decentralized}, but this area of research could use more attention. Chapter 2 will examine two distinct ways that we can extend the definition of normal form potential games to the stochastic setting. Chapter 3 presents extensions of two normal form game learning methods, joint strategy fictitious play (JSFP) with inertia and log-linear learning (LLL), to these stochastic games.  


The preceding discussion suggests a {\em prescriptive} motivation for studying stochastic games, with a view towards applying stochastic game learning methods as a computational tool. However, there is also a {\em descriptive} motivation for understanding such methods. There is an enormous amount of research in psychology and neuroscience which suggests that mammalian learning resembles a form of reinforcement learning \cite{zald2004dopamine, pessiglione2006dopamine, rescorla1972theory}. On the other hand, we know that humans and animals learn in an environment filled with other learning agents. A careful study of learning in stochastic games may uncover explanations for observed animal and human behavior.


In conclusion, the study of learning in stochastic games has at least two distinct motivations. First, there are real world single-agent problems that may become more computationally tractible when decomposed into several simpler interrelated learning problems. Second, understanding the possibilities and limitations of learning in stochastic games may help us better understand animal and human behavior. Despite these motivations, theoretical studies on learning in stochastic games have remained scarce. Possible directions of research include identifying subclasses of stochastic games amenable to learning algorithms, proving convergence results in such games, and characterizing various equilibria sets for such games. This thesis embarks on such a study by extending potential games in two different ways into the class of {\em stochastic potential games} (SPGs) and {\em stochastic global potential games} (SGPGs). For these classes of games we study their structure, equilibria sets, and identify several convergent learning algorithms. In the final chapter we shift to an empirical study of the power of applying DeepRL to multi-agent decision problems.















