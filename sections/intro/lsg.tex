\section{Learning in Stochastic Games}

The stochastic game framework is a very flexible model for studying learning because stochastic games encompass and extend two already large bodies of problems: the Markov decision process framework of single agent learning and the normal form game framework of strategic multi-player interaction. Researchers at the turn of the century realized the possibility of approaching game theoretic problems with reinforcement learning solutions \cite{littman1994markov, hu2003nash, bowling2001rational}. However, it became clear that one could not simply use existing reinforcement learning methods in stochastic games and expect any type of convergence to equilibrium \cite{bowling2000convergence} despite earlier claims to the contrary \cite{claus1998dynamics}. Ultimately this period of research produced a few convergent "reinforcement style" algorithms in stochastic games \cite{bowling2001rational, littman1994markov, hu2003nash, littman2001value}, however their applicability was limited. For example, some apply only in zero sum games, 2-person 2 action games, or require specific conditions apply throughout the learning process. 

The question of how to learn in stochastic games remained somewhat dormant from 2005 until 2015. With the incredible success of combining reinforcement learning and neural networks, some researchers began taking this empirical success and applying it to multi-agent environments. In fact, one of the monumental successes of DeepRL, Alphago \cite{silver2016mastering, alphazero}, was a result of applying DeepRL in a zero sum game, {\em not} a stationary single-agent environment. More concretely multi-agent DeepRL papers include \cite{foerster2018counterfactual, rashid2018qmix, balduzzi2018mechanics, yang2018mean, tampuu2017multiagent, omidshafiei2017deep, van2016coordinated, lowe2017multi}. These works are primarily empirical, but inspired by older, theoretical work in game theory and reinforcement learning. For example, counterfactual policy gradients \cite{foerster2018counterfactual} are closely related to the wonderful life utility in collective intelligence \cite{wolpert2002optimal}, and mean field Q-learning \cite{yang2018mean} is informed by the study of mean field games \cite{lasry2007mean}.

There has also been some recent theoretical work on learning on stochastic games \cite{zerosumstochastic, arslan2016decentralized}. These two works study learning in stochastic zero-sum games  \cite{zerosumstochastic} and stochastic team games \cite{arslan2016decentralized}. The work in chapters 2 and 3 most naturally belong in this family of work, as we define two other classes of stochastic games and perform a theoretical study of learning methods in these games.
